<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-06-27T17:24:14+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chandresh Kumar Maurya</title><subtitle>Assistant Professor at IIT Indore</subtitle><author><name>Chandresh Kumar Maurya</name><email>ckm.jnu@gmail.com</email></author><entry><title type="html">Beam search decoding in 5 mins</title><link href="http://localhost:4000/BeamSearch/" rel="alternate" type="text/html" title="Beam search decoding in 5 mins" /><published>2019-12-11T00:00:00+05:30</published><updated>2019-12-11T00:00:00+05:30</updated><id>http://localhost:4000/BeamSearch</id><content type="html" xml:base="http://localhost:4000/BeamSearch/">&lt;h3 id=&quot;beam-search&quot;&gt;Beam Search&lt;/h3&gt;

&lt;p&gt;Over the years, beam search has become one of the most popular technique for decoding a sequence at test time. Most of the deep learning papers using sequence models use beam search to find alternative translations, part-of-speech tags, named-entities etc. In the post, I am going to cover beam search.&lt;/p&gt;

&lt;p&gt;Beam search is one of the many techniques for searching a graph such as &lt;em&gt;breadth-first-search, depth-first-search, best-first-search, hill-climbing&lt;/em&gt; . You can think of it as a constrained version of the bread-first search where we explore only &lt;em&gt;k&lt;/em&gt; best neighbours of the current node. Here , &lt;em&gt;k&lt;/em&gt; is called &lt;strong&gt;Beam Width&lt;/strong&gt;. The discussion here follows in terms of label prediction in the sequence models though it can be applied to other domains without loss of generality.&lt;/p&gt;

&lt;p&gt;Imagine that you got the following output from one of your LSTM cell.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-Markdown&quot;&gt;$$
\[\begin{bmatrix}
1&amp;amp;2&amp;amp;3\\
2&amp;amp;1&amp;amp;3\\
3&amp;amp;1&amp;amp;2\\
\end{bmatrix}\]
$$
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can read about beam search &lt;a href=&quot;http://www.qinuu.com/2019/04/23/3-sequence-models-attention-mechanism/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/#comment-514613&quot;&gt;here&lt;/a&gt;. What I will give you the code.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;beam_search_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# first convert logits to probabilites so that all numbers are +ve
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
    &lt;span class=&quot;c1&quot;&gt;# walk over each step in sequence
&lt;/span&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;all_candidates&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# expand each current candidate
&lt;/span&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#             for j in range(len(row)): # instead of exploring all the labels, explore only k best at the current time
&lt;/span&gt;            &lt;span class=&quot;c1&quot;&gt;# select k best
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;best_k&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;argsort&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# explore k best
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;best_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;candidate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;math&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])]&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;all_candidates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;candidate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# order all candidates by score
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;ordered&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;all_candidates&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reverse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# select k best
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ordered&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequences&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Above code expects that your data is just logits coming from the output of RNN/LSTM. It will compute the softmax first. Then, at each step, it iteratively expands each node and choosing the best K before expending.  If you call the above function on the sample data, you get the below output.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([[&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beam_search_decoder&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;gt;[[[2, 2, 0], &amp;lt;tf.Tensor: id=10511, shape=(), dtype=float64, numpy=-1.2228178933331413&amp;gt;], [[2, 2, 2], &amp;lt;tf.Tensor: id=10505, shape=(), dtype=float64, numpy=-2.2228178933331413&amp;gt;], [[2, 0, 0], &amp;lt;tf.Tensor: id=10529, shape=(), dtype=float64, numpy=-2.2228178933331413&amp;gt;]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name>Chandresh Kumar Maurya</name><email>ckm.jnu@gmail.com</email></author><category term="Beam Search" /><summary type="html">Beam Search</summary></entry><entry><title type="html">Dependency Parsing in 5 mins</title><link href="http://localhost:4000/DependencyParsing/" rel="alternate" type="text/html" title="Dependency Parsing in 5 mins" /><published>2019-12-01T00:00:00+05:30</published><updated>2019-12-01T00:00:00+05:30</updated><id>http://localhost:4000/DependencyParsing</id><content type="html" xml:base="http://localhost:4000/DependencyParsing/">&lt;p&gt;&lt;strong&gt;TL;DR&lt;/strong&gt;
Dependency parsing in natural language processing is a parsing technique which gives relations (labels) to words via head-dependent relationship. Contrary to constituent-based parsing where relationship between words is embedded in deep, in dependency graph (defined below), relationship are flat. More formally, dependency parsing  creates connected, directed, and labeled graph \(G=(V,E)\) where \(V\) is the set of words and \(E\) is the set of edges consisting of ordered pair of verticies. The graph (dependency tree to be more precise)   has following properties.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;There is &lt;strong&gt;root&lt;/strong&gt; node which does not have any parent associated to it.&lt;/li&gt;
  &lt;li&gt;Every vertex in the graph has an incoming arc except the root.&lt;/li&gt;
  &lt;li&gt;There is a unique path from the root to every other vertics.&lt;/li&gt;
&lt;/ol&gt;</content><author><name>Chandresh Kumar Maurya</name><email>ckm.jnu@gmail.com</email></author><category term="Dependency Parsing" /><summary type="html">TL;DR Dependency parsing in natural language processing is a parsing technique which gives relations (labels) to words via head-dependent relationship. Contrary to constituent-based parsing where relationship between words is embedded in deep, in dependency graph (defined below), relationship are flat. More formally, dependency parsing creates connected, directed, and labeled graph \(G=(V,E)\) where \(V\) is the set of words and \(E\) is the set of edges consisting of ordered pair of verticies. The graph (dependency tree to be more precise) has following properties. There is root node which does not have any parent associated to it. Every vertex in the graph has an incoming arc except the root. There is a unique path from the root to every other vertics.</summary></entry><entry><title type="html">Reservoir Sampling</title><link href="http://localhost:4000/perceptron/" rel="alternate" type="text/html" title="Reservoir Sampling" /><published>2019-09-25T00:00:00+05:30</published><updated>2019-09-25T00:00:00+05:30</updated><id>http://localhost:4000/perceptron</id><content type="html" xml:base="http://localhost:4000/perceptron/">&lt;p&gt;Under Contruction…&lt;/p&gt;</content><author><name>Chandresh Kumar Maurya</name><email>ckm.jnu@gmail.com</email></author><category term="stream mining" /><summary type="html">Under Contruction…</summary></entry></feed>